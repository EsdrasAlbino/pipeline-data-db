{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuqGfgwfrwl1"
   },
   "source": [
    "# 📊 Pipeline de Dados - Acidentes de Trânsito (2019-2021)\n",
    "\n",
    "## 🛠️ Imports e Configurações Iniciais\n",
    "\n",
    "Este notebook implementa pipelines ETL e ELT para integração de dados de acidentes de trânsito de três anos consecutivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4665,
     "status": "ok",
     "timestamp": 1754516960194,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "lh1NwZWvrvnj",
    "outputId": "384102db-5596-46b5-d60f-1f69e421c9d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "619.88s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/ec/d3/3c37cb724d76a841f14b8f5fe57e5e3645207cc67370e4f84717e8bb7657/pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b7/13/e792d7209261afb0c9f4759ffef6135b35c77c6349a151f488f531d13595/numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b7/13/e792d7209261afb0c9f4759ffef6135b35c77c6349a151f488f531d13595/numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sqlalchemy\n",
      "  Obtaining dependency information for sqlalchemy from https://files.pythonhosted.org/packages/66/17/19be542fe9dd64a766090e90e789e86bdaa608affda6b3c1e118a25a2509/sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting sqlalchemy\n",
      "  Obtaining dependency information for sqlalchemy from https://files.pythonhosted.org/packages/66/17/19be542fe9dd64a766090e90e789e86bdaa608affda6b3c1e118a25a2509/sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting openpyxl\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl.metadata\n",
      "Collecting openpyxl\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl.metadata\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sqlalchemy) (4.14.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sqlalchemy) (4.14.1)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/c1/8b/5fe2cc11fee489817272089c4203e679c63b570a5aaeb18d852ae3cbba6a/et_xmlfile-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/c1/8b/5fe2cc11fee489817272089c4203e679c63b570a5aaeb18d852ae3cbba6a/et_xmlfile-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/2.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mDownloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/250.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: pytz, tzdata, sqlalchemy, numpy, et-xmlfile, pandas, openpyxl\n",
      "Installing collected packages: pytz, tzdata, sqlalchemy, numpy, et-xmlfile, pandas, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 numpy-2.3.2 openpyxl-3.1.5 pandas-2.3.1 pytz-2025.2 sqlalchemy-2.0.42 tzdata-2025.2\n",
      "Successfully installed et-xmlfile-2.0.0 numpy-2.3.2 openpyxl-3.1.5 pandas-2.3.1 pytz-2025.2 sqlalchemy-2.0.42 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Todas as bibliotecas foram importadas com sucesso!\n",
      "📦 Pandas versão: 2.3.1\n",
      "📦 SQLAlchemy versão: 2.0.42\n",
      "✅ Todas as bibliotecas foram importadas com sucesso!\n",
      "📦 Pandas versão: 2.3.1\n",
      "📦 SQLAlchemy versão: 2.0.42\n"
     ]
    }
   ],
   "source": [
    "# Instalar pacotes necessários no ambiente do notebook\n",
    "%pip install pandas numpy sqlalchemy openpyxl\n",
    "\n",
    "# Bibliotecas para manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para banco de dados\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "print(\"✅ Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"📦 Pandas versão: {pd.__version__}\")\n",
    "print(f\"📦 SQLAlchemy versão: {sqlalchemy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2041,
     "status": "ok",
     "timestamp": 1754516962236,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "AaSDDO64KtzF",
    "outputId": "95ec8ec2-dfe1-4a5f-8bcf-6bfe25a8a4e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o processo de ETL ---\n",
      "Etapa 1: Extraindo dados dos arquivos CSV...\n",
      "Dados extraídos com sucesso!\n",
      "\n",
      "Etapa 2: Transformando os dados...\n",
      "- Nomes de colunas padronizados.\n",
      "- Colunas harmonizadas entre os datasets.\n",
      "- Colunas de data e hora convertidas para o formato timestamp.\n",
      "- Coluna 'ano' criada.\n",
      "- Tipos de dados das colunas numéricas padronizados para inteiro.\n",
      "- Endereços padronizados.\n",
      "\n",
      "Etapa 3: Carregando os dados transformados...\n",
      "- DataFrames unificados com sucesso.\n",
      "\n",
      "--- Processo de ETL Concluído! ---\n",
      "A base de dados unificada foi salva como: 'acidentes_unificados_2019-2021.csv'\n",
      "Total de registros na base unificada: 18534\n",
      "\n",
      "Amostra dos 5 primeiros registros da base final:\n",
      "    ano           timestamp natureza_acidente    situacao       bairro  \\\n",
      "0  2019 2019-01-01 00:41:00        SEM VÍTIMA  FINALIZADA        IPSEP   \n",
      "1  2019 2019-01-01 01:37:00        SEM VÍTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019 2019-01-01 14:20:00        SEM VÍTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019 2019-01-01 02:53:00        SEM VÍTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019 2019-01-01 08:17:00        COM VÍTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco  numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE      -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA      -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA      -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR     100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS      63                 Desconhecido   \n",
      "\n",
      "                                         complemento bairro_cruzamento  ...  \\\n",
      "0                             LADO OPOSTO AO Nº 3257             IPSEP  ...   \n",
      "1                                       Desconhecido        BOA VIAGEM  ...   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...        BOA VIAGEM  ...   \n",
      "3                     EM FRENTE A ART LED ILUMINAÇÃO       IMBIRIBEIRA  ...   \n",
      "4                            ED. JARDINS DA JAQUEIRA          JAQUEIRA  ...   \n",
      "\n",
      "       sinalizacao condicao_via  conservacao_via     ponto_controle  \\\n",
      "0  Perfeito estado         Seca  Perfeito estado         Não existe   \n",
      "1  Perfeito estado         Seca  Perfeito estado  Faixa de pedestre   \n",
      "2              NaN          NaN              NaN                NaN   \n",
      "3              NaN          NaN              NaN                NaN   \n",
      "4  Perfeito estado         Seca  Perfeito estado         Não existe   \n",
      "\n",
      "   situacao_placa  velocidade_max_via  mao_direcao      divisao_via1  \\\n",
      "0   Não há placas             60 km/h        Única  Faixa seccionada   \n",
      "1   Não há placas                 NaN        Única        Não existe   \n",
      "2             NaN                 NaN          NaN               NaN   \n",
      "3             NaN                 NaN          NaN               NaN   \n",
      "4   Não há placas             40 km/h        Única  Faixa seccionada   \n",
      "\n",
      "   divisao_via2  divisao_via3  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "Informações da base final:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18534 entries, 0 to 18533\n",
      "Data columns (total 37 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   ano                        18534 non-null  int64         \n",
      " 1   timestamp                  18516 non-null  datetime64[ns]\n",
      " 2   natureza_acidente          18529 non-null  object        \n",
      " 3   situacao                   18529 non-null  object        \n",
      " 4   bairro                     18347 non-null  object        \n",
      " 5   endereco                   18534 non-null  object        \n",
      " 6   numero                     18534 non-null  int64         \n",
      " 7   detalhe_endereco_acidente  18534 non-null  object        \n",
      " 8   complemento                18534 non-null  object        \n",
      " 9   bairro_cruzamento          18341 non-null  object        \n",
      " 10  num_semaforo               18534 non-null  int64         \n",
      " 11  sentido_via                12651 non-null  object        \n",
      " 12  tipo                       18476 non-null  object        \n",
      " 13  auto                       18534 non-null  int64         \n",
      " 14  moto                       18534 non-null  int64         \n",
      " 15  ciclom                     18534 non-null  int64         \n",
      " 16  ciclista                   18534 non-null  int64         \n",
      " 17  pedestre                   18534 non-null  int64         \n",
      " 18  onibus                     18534 non-null  int64         \n",
      " 19  caminhao                   18534 non-null  int64         \n",
      " 20  viatura                    18534 non-null  int64         \n",
      " 21  outros                     18534 non-null  int64         \n",
      " 22  vitimas                    18534 non-null  int64         \n",
      " 23  vitimasfatais              18534 non-null  int64         \n",
      " 24  acidente_verificado        14266 non-null  object        \n",
      " 25  tempo_clima                14879 non-null  object        \n",
      " 26  situacao_semaforo          14686 non-null  object        \n",
      " 27  sinalizacao                14589 non-null  object        \n",
      " 28  condicao_via               14830 non-null  object        \n",
      " 29  conservacao_via            14606 non-null  object        \n",
      " 30  ponto_controle             13516 non-null  object        \n",
      " 31  situacao_placa             13455 non-null  object        \n",
      " 32  velocidade_max_via         4609 non-null   object        \n",
      " 33  mao_direcao                14569 non-null  object        \n",
      " 34  divisao_via1               14043 non-null  object        \n",
      " 35  divisao_via2               1449 non-null   object        \n",
      " 36  divisao_via3               248 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(14), object(22)\n",
      "memory usage: 5.2+ MB\n",
      "\n",
      "Etapa 3.3: Inserindo os dados no banco de dados...\n",
      "\n",
      "--- Processo de ETL Concluído! ---\n",
      "A base de dados unificada foi salva como: 'acidentes_unificados_2019-2021.csv'\n",
      "Total de registros na base unificada: 18534\n",
      "\n",
      "Amostra dos 5 primeiros registros da base final:\n",
      "    ano           timestamp natureza_acidente    situacao       bairro  \\\n",
      "0  2019 2019-01-01 00:41:00        SEM VÍTIMA  FINALIZADA        IPSEP   \n",
      "1  2019 2019-01-01 01:37:00        SEM VÍTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019 2019-01-01 14:20:00        SEM VÍTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019 2019-01-01 02:53:00        SEM VÍTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019 2019-01-01 08:17:00        COM VÍTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco  numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE      -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA      -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA      -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR     100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS      63                 Desconhecido   \n",
      "\n",
      "                                         complemento bairro_cruzamento  ...  \\\n",
      "0                             LADO OPOSTO AO Nº 3257             IPSEP  ...   \n",
      "1                                       Desconhecido        BOA VIAGEM  ...   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...        BOA VIAGEM  ...   \n",
      "3                     EM FRENTE A ART LED ILUMINAÇÃO       IMBIRIBEIRA  ...   \n",
      "4                            ED. JARDINS DA JAQUEIRA          JAQUEIRA  ...   \n",
      "\n",
      "       sinalizacao condicao_via  conservacao_via     ponto_controle  \\\n",
      "0  Perfeito estado         Seca  Perfeito estado         Não existe   \n",
      "1  Perfeito estado         Seca  Perfeito estado  Faixa de pedestre   \n",
      "2              NaN          NaN              NaN                NaN   \n",
      "3              NaN          NaN              NaN                NaN   \n",
      "4  Perfeito estado         Seca  Perfeito estado         Não existe   \n",
      "\n",
      "   situacao_placa  velocidade_max_via  mao_direcao      divisao_via1  \\\n",
      "0   Não há placas             60 km/h        Única  Faixa seccionada   \n",
      "1   Não há placas                 NaN        Única        Não existe   \n",
      "2             NaN                 NaN          NaN               NaN   \n",
      "3             NaN                 NaN          NaN               NaN   \n",
      "4   Não há placas             40 km/h        Única  Faixa seccionada   \n",
      "\n",
      "   divisao_via2  divisao_via3  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "Informações da base final:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18534 entries, 0 to 18533\n",
      "Data columns (total 37 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   ano                        18534 non-null  int64         \n",
      " 1   timestamp                  18516 non-null  datetime64[ns]\n",
      " 2   natureza_acidente          18529 non-null  object        \n",
      " 3   situacao                   18529 non-null  object        \n",
      " 4   bairro                     18347 non-null  object        \n",
      " 5   endereco                   18534 non-null  object        \n",
      " 6   numero                     18534 non-null  int64         \n",
      " 7   detalhe_endereco_acidente  18534 non-null  object        \n",
      " 8   complemento                18534 non-null  object        \n",
      " 9   bairro_cruzamento          18341 non-null  object        \n",
      " 10  num_semaforo               18534 non-null  int64         \n",
      " 11  sentido_via                12651 non-null  object        \n",
      " 12  tipo                       18476 non-null  object        \n",
      " 13  auto                       18534 non-null  int64         \n",
      " 14  moto                       18534 non-null  int64         \n",
      " 15  ciclom                     18534 non-null  int64         \n",
      " 16  ciclista                   18534 non-null  int64         \n",
      " 17  pedestre                   18534 non-null  int64         \n",
      " 18  onibus                     18534 non-null  int64         \n",
      " 19  caminhao                   18534 non-null  int64         \n",
      " 20  viatura                    18534 non-null  int64         \n",
      " 21  outros                     18534 non-null  int64         \n",
      " 22  vitimas                    18534 non-null  int64         \n",
      " 23  vitimasfatais              18534 non-null  int64         \n",
      " 24  acidente_verificado        14266 non-null  object        \n",
      " 25  tempo_clima                14879 non-null  object        \n",
      " 26  situacao_semaforo          14686 non-null  object        \n",
      " 27  sinalizacao                14589 non-null  object        \n",
      " 28  condicao_via               14830 non-null  object        \n",
      " 29  conservacao_via            14606 non-null  object        \n",
      " 30  ponto_controle             13516 non-null  object        \n",
      " 31  situacao_placa             13455 non-null  object        \n",
      " 32  velocidade_max_via         4609 non-null   object        \n",
      " 33  mao_direcao                14569 non-null  object        \n",
      " 34  divisao_via1               14043 non-null  object        \n",
      " 35  divisao_via2               1449 non-null   object        \n",
      " 36  divisao_via3               248 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(14), object(22)\n",
      "memory usage: 5.2+ MB\n",
      "\n",
      "Etapa 3.3: Inserindo os dados no banco de dados...\n",
      "- Dados inseridos com sucesso na tabela 'acidentes' do banco SQLite (arquivo 'banco_etl.db').\n",
      "- Dados inseridos com sucesso na tabela 'acidentes' do banco SQLite (arquivo 'banco_etl.db').\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def run_etl_pipeline():\n",
    "    \"\"\"\n",
    "    Executa o pipeline completo de ETL para os dados de acidentes de 2019, 2020 e 2021.\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando o processo de ETL ---\")\n",
    "\n",
    "    # --- 1. EXTRAÇÃO (Extract) ---\n",
    "    # Carregando os três conjuntos de dados a partir dos arquivos CSV.\n",
    "    try:\n",
    "        print(\"Etapa 1: Extraindo dados dos arquivos CSV...\")\n",
    "        df_2019 = pd.read_csv('data/acidentes-2019.csv', delimiter=';')\n",
    "        df_2020 = pd.read_csv('data/acidentes_2020-novo.csv', delimiter=';')\n",
    "        df_2021 = pd.read_csv('data/acidentes2021.csv', delimiter=';')\n",
    "        print(\"Dados extraídos com sucesso!\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRO: Arquivo não encontrado. Verifique se os nomes dos arquivos estão corretos. Detalhes: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. TRANSFORMAÇÃO (Transform) ---\n",
    "    print(\"\\nEtapa 2: Transformando os dados...\")\n",
    "\n",
    "    # 2.1 Padronização dos nomes das colunas\n",
    "    # O dataset de 2019 tem a coluna 'DATA' em maiúsculo. Vamos padronizar para 'data'.\n",
    "    df_2019.rename(columns={'DATA': 'data'}, inplace=True)\n",
    "    print(\"- Nomes de colunas padronizados.\")\n",
    "\n",
    "    # 2.2 Harmonização das colunas (removendo colunas inconsistentes)\n",
    "    # Colunas presentes em 2019 mas não em 2020/2021\n",
    "    cols_to_drop_2019 = ['endereco_cruzamento', 'numero_cruzamento', 'referencia_cruzamento', 'descricao']\n",
    "    df_2019.drop(columns=[col for col in cols_to_drop_2019 if col in df_2019.columns], inplace=True)\n",
    "\n",
    "    # Coluna 'descricao' presente em 2020 mas não em 2021\n",
    "    if 'descricao' in df_2020.columns:\n",
    "        df_2020.drop(columns=['descricao'], inplace=True)\n",
    "\n",
    "    # A coluna 'tipo' em 2021 está no lugar de 'descricao', mas para unificar vamos manter apenas as colunas em comum.\n",
    "    # A base de 2021 não tem a coluna 'descricao', então não é necessário fazer nada.\n",
    "\n",
    "    print(\"- Colunas harmonizadas entre os datasets.\")\n",
    "\n",
    "    # Lista dos dataframes para aplicar as transformações em lote\n",
    "    dataframes = [df_2019, df_2020, df_2021]\n",
    "    ano_inicio = 2019 # Precisa ser alterado caso entre anos anteriores no data set\n",
    "\n",
    "    # 2.3 Transformação de data e hora\n",
    "    for i, df in enumerate(dataframes):\n",
    "        ano = ano_inicio + i\n",
    "        # Corrigindo valores de hora inválidos como '24:00:00'\n",
    "        df['hora'] = df['hora'].str.replace('24:00:00', '00:00:00', regex=False)\n",
    "        # Combinando 'data' e 'hora' em uma única coluna 'timestamp'\n",
    "        # 'coerce' transforma datas inválidas em NaT (Not a Time)\n",
    "        df['timestamp'] = pd.to_datetime(df['data'] + ' ' + df['hora'], errors='coerce')\n",
    "        # Adicionando a coluna 'ano'\n",
    "        df['ano'] = ano\n",
    "        # Removendo as colunas originais\n",
    "        df.drop(columns=['data', 'hora'], inplace=True)\n",
    "\n",
    "    print(\"- Colunas de data e hora convertidas para o formato timestamp.\")\n",
    "    print(\"- Coluna 'ano' criada.\")\n",
    "\n",
    "    # 2.4 Padronização de tipos de dados numéricos\n",
    "    colunas_numericas = [\n",
    "        'auto', 'moto', 'ciclom', 'ciclista', 'pedestre', 'onibus',\n",
    "        'caminhao', 'viatura', 'outros', 'vitimas', 'vitimasfatais'\n",
    "    ]\n",
    "\n",
    "    for df in dataframes:\n",
    "        for col in colunas_numericas:\n",
    "            if col in df.columns:\n",
    "                # Converte para numérico, tratando erros, e preenche NaNs com 0\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "                # Converte para inteiro\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "    # Tratamento específico para 'num_semaforo' que tem tipos diferentes\n",
    "    for df in dataframes:\n",
    "        if 'num_semaforo' in df.columns:\n",
    "            df['num_semaforo'] = pd.to_numeric(df['num_semaforo'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    print(\"- Tipos de dados das colunas numéricas padronizados para inteiro.\")\n",
    "\n",
    "    # Tratamento de endereco\n",
    "    for df in dataframes:\n",
    "        if 'endereco' in df.columns:\n",
    "            df['endereco'] = df['endereco'].str.strip().replace(r'^\\s*$', np.nan, regex=True).fillna('Desconhecido')\n",
    "        if 'detalhe_endereco_acidente' in df.columns:\n",
    "            df['detalhe_endereco_acidente'] = df['detalhe_endereco_acidente'].str.strip().replace(r'^\\s*$', np.nan, regex=True).fillna('Desconhecido')\n",
    "        if 'complemento' in df.columns:\n",
    "            df['complemento'] = df['complemento'].str.strip().replace(r'^\\s*$', np.nan, regex=True).fillna('Desconhecido')\n",
    "        if 'numero' in df.columns:\n",
    "            df['numero'] = pd.to_numeric(df['numero'], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "    print(\"- Endereços padronizados.\")\n",
    "\n",
    "    # --- 3. CARGA (Load) ---\n",
    "    print(\"\\nEtapa 3: Carregando os dados transformados...\")\n",
    "\n",
    "    # 3.1 Unificação dos DataFrames\n",
    "    # Concatenando os três dataframes em um só\n",
    "    df_unificado = pd.concat(dataframes, ignore_index=True)\n",
    "    print(\"- DataFrames unificados com sucesso.\")\n",
    "\n",
    "    # Reordenando colunas para colocar 'ano' e 'timestamp' no início\n",
    "    cols = ['ano', 'timestamp'] + [col for col in df_unificado.columns if col not in ['ano', 'timestamp']]\n",
    "    df_unificado = df_unificado[cols]\n",
    "\n",
    "    # 3.2 Salvando o resultado em um novo arquivo CSV\n",
    "    output_filename = 'acidentes_unificados_2019-2021.csv'\n",
    "    df_unificado.to_csv(output_filename, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "    print(f\"\\n--- Processo de ETL Concluído! ---\")\n",
    "    print(f\"A base de dados unificada foi salva como: '{output_filename}'\")\n",
    "    print(f\"Total de registros na base unificada: {len(df_unificado)}\")\n",
    "    print(\"\\nAmostra dos 5 primeiros registros da base final:\")\n",
    "    print(df_unificado.head())\n",
    "    print(\"\\nInformações da base final:\")\n",
    "    df_unificado.info()\n",
    "\n",
    "    # 3.3 Inserindo no banco de dados\n",
    "    print(\"\\nEtapa 3.3: Inserindo os dados no banco de dados...\")\n",
    "\n",
    "    # criando um arquivo .db\n",
    "    nome_do_banco_de_dados2 = \"banco_etl.db\"\n",
    "    nome_tabela = 'acidentes'\n",
    "\n",
    "    engine = create_engine(f'sqlite:///{nome_do_banco_de_dados2}')\n",
    "\n",
    "    df_unificado.to_sql(nome_tabela,\n",
    "                        con=engine,\n",
    "                        if_exists='replace',\n",
    "                        index=False)\n",
    "\n",
    "    print(f\"- Dados inseridos com sucesso na tabela '{nome_tabela}' do banco SQLite (arquivo '{nome_do_banco_de_dados2}').\")\n",
    "# Executando a função principal do pipeline\n",
    "run_etl_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4yTS38lrPrG"
   },
   "source": [
    "# ETAPA ELT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9914,
     "status": "ok",
     "timestamp": 1754516972151,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "Oc3Ko0KurZ78",
    "outputId": "45a21907-1246-4ab1-d495-cfa42d6d8dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Limpando banco de dados existente para evitar conflitos de schema...\n",
      "--- Iniciando o processo de ELT ---\n",
      "Processando arquivo: data/acidentes2019.csv\n",
      "--- Iniciando o processo de ELT ---\n",
      "Processando arquivo: data/acidentes2019.csv\n",
      "- Arquivo data/acidentes2019.csv carregado com sucesso (12062 registros)\n",
      "Processando arquivo: data/acidentes2020.csv\n",
      "- Arquivo data/acidentes2020.csv carregado com sucesso (4092 registros)\n",
      "Processando arquivo: data/acidentes2021.csv\n",
      "- Arquivo data/acidentes2021.csv carregado com sucesso (2380 registros)\n",
      "\n",
      "🔄 Iniciando transformações no banco de dados (ELT)...\n",
      "Total de registros carregados: 18534\n",
      "- Arquivo data/acidentes2019.csv carregado com sucesso (12062 registros)\n",
      "Processando arquivo: data/acidentes2020.csv\n",
      "- Arquivo data/acidentes2020.csv carregado com sucesso (4092 registros)\n",
      "Processando arquivo: data/acidentes2021.csv\n",
      "- Arquivo data/acidentes2021.csv carregado com sucesso (2380 registros)\n",
      "\n",
      "🔄 Iniciando transformações no banco de dados (ELT)...\n",
      "Total de registros carregados: 18534\n",
      "- Coluna 'timestamp' preenchida com DATA e hora.\n",
      "- Endereço e número tratados.\n",
      "- Coluna 'timestamp' preenchida com DATA e hora.\n",
      "- Endereço e número tratados.\n",
      "- Colunas numéricas padronizadas.\n",
      "- Coluna 'tipo_acidente' criada.\n",
      "- Colunas numéricas padronizadas.\n",
      "- Coluna 'tipo_acidente' criada.\n",
      "- Colunas de texto padronizadas.\n",
      "- Coluna 'id_linha' adicionada e preenchida.\n",
      "- Colunas de texto padronizadas.\n",
      "- Coluna 'id_linha' adicionada e preenchida.\n",
      "- Velocidade máxima da via tratada.\n",
      "\n",
      "--- Verificação Final do Processo ELT ---\n",
      "A tabela final 'dados_brutos_sinistros' contém um total de 18534 registros.\n",
      "\n",
      "Contagem de registros por ano de origem do dado:\n",
      "   ano_do_dado  total_de_linhas\n",
      "0         2019            12062\n",
      "1         2020             4092\n",
      "2         2021             2380\n",
      "\n",
      "DataFrame final criado com 18534 registros e 45 colunas.\n",
      "\n",
      "Primeiras 5 linhas:\n",
      "         DATA      hora natureza_acidente    situacao       bairro  \\\n",
      "0  2019-01-01  00:41:00        SEM VÍTIMA  FINALIZADA        IPSEP   \n",
      "1  2019-01-01  01:37:00        SEM VÍTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019-01-01  14:20:00        SEM VÍTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019-01-01  02:53:00        SEM VÍTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019-01-01  08:17:00        COM VÍTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE     -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA     -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA     -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR    100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS     63                 Desconhecido   \n",
      "\n",
      "                                         complemento  \\\n",
      "0                             LADO OPOSTO AO Nº 3257   \n",
      "1                                       Desconhecido   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...   \n",
      "3                     EM FRENTE A ART LED ILUMINAÇÃO   \n",
      "4                            ED. JARDINS DA JAQUEIRA   \n",
      "\n",
      "               endereco_cruzamento  ... situacao_placa velocidade_max_via  \\\n",
      "0                        AV RECIFE  ...  Não há placas                 60   \n",
      "1       RUA PADRE BERNADINO PESSOA  ...  Não há placas                 -1   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA  ...   Desconhecido                 -1   \n",
      "3            AV GENERAL MAC ARTHUR  ...   Desconhecido                 -1   \n",
      "4                   RUA TITO ROSAS  ...  Não há placas                 40   \n",
      "\n",
      "    mao_direcao      divisao_via1  divisao_via2  divisao_via3 ano_do_dado  \\\n",
      "0         Única  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "1         Única        Não existe  Desconhecido  Desconhecido        2019   \n",
      "2  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "3  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "4         Única  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "\n",
      "             TimeStamp  tipo_acidente  id_linha  \n",
      "0  2019-01-01 00:41:00         Outros         1  \n",
      "1  2019-01-01 01:37:00         Outros         2  \n",
      "2  2019-01-01 14:20:00         Outros         3  \n",
      "3  2019-01-01 02:53:00         Outros         4  \n",
      "4  2019-01-01 08:17:00   Com ciclista         5  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "- Velocidade máxima da via tratada.\n",
      "\n",
      "--- Verificação Final do Processo ELT ---\n",
      "A tabela final 'dados_brutos_sinistros' contém um total de 18534 registros.\n",
      "\n",
      "Contagem de registros por ano de origem do dado:\n",
      "   ano_do_dado  total_de_linhas\n",
      "0         2019            12062\n",
      "1         2020             4092\n",
      "2         2021             2380\n",
      "\n",
      "DataFrame final criado com 18534 registros e 45 colunas.\n",
      "\n",
      "Primeiras 5 linhas:\n",
      "         DATA      hora natureza_acidente    situacao       bairro  \\\n",
      "0  2019-01-01  00:41:00        SEM VÍTIMA  FINALIZADA        IPSEP   \n",
      "1  2019-01-01  01:37:00        SEM VÍTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019-01-01  14:20:00        SEM VÍTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019-01-01  02:53:00        SEM VÍTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019-01-01  08:17:00        COM VÍTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE     -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA     -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA     -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR    100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS     63                 Desconhecido   \n",
      "\n",
      "                                         complemento  \\\n",
      "0                             LADO OPOSTO AO Nº 3257   \n",
      "1                                       Desconhecido   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...   \n",
      "3                     EM FRENTE A ART LED ILUMINAÇÃO   \n",
      "4                            ED. JARDINS DA JAQUEIRA   \n",
      "\n",
      "               endereco_cruzamento  ... situacao_placa velocidade_max_via  \\\n",
      "0                        AV RECIFE  ...  Não há placas                 60   \n",
      "1       RUA PADRE BERNADINO PESSOA  ...  Não há placas                 -1   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA  ...   Desconhecido                 -1   \n",
      "3            AV GENERAL MAC ARTHUR  ...   Desconhecido                 -1   \n",
      "4                   RUA TITO ROSAS  ...  Não há placas                 40   \n",
      "\n",
      "    mao_direcao      divisao_via1  divisao_via2  divisao_via3 ano_do_dado  \\\n",
      "0         Única  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "1         Única        Não existe  Desconhecido  Desconhecido        2019   \n",
      "2  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "3  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "4         Única  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "\n",
      "             TimeStamp  tipo_acidente  id_linha  \n",
      "0  2019-01-01 00:41:00         Outros         1  \n",
      "1  2019-01-01 01:37:00         Outros         2  \n",
      "2  2019-01-01 14:20:00         Outros         3  \n",
      "3  2019-01-01 02:53:00         Outros         4  \n",
      "4  2019-01-01 08:17:00   Com ciclista         5  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "#INICIANDO A SUBIDA DOS DADOS BRUTOS DE 2019 ATÉ 2021, Utilizando as bibliotecas panda e sqlalchemy\n",
    "\n",
    "# --- CONFIGURAÇÃO---\n",
    "# Nome dos arquivos CSV corrigidos para corresponder aos arquivos reais\n",
    "lista_arquivos_csv = ['data/acidentes2019.csv', 'data/acidentes2020.csv', 'data/acidentes2021.csv']\n",
    "\n",
    "# Nome do arquivo do banco de dados\n",
    "nome_do_banco_de_dados = 'banco_elt.db'\n",
    "\n",
    "# Nome da tabela onde os dados brutos serão guardados\n",
    "nome_da_tabela_consolidada = 'dados_brutos_sinistros'\n",
    "\n",
    "# Conectar ao banco de dados\n",
    "engine = create_engine(f'sqlite:///{nome_do_banco_de_dados}')\n",
    "\n",
    "# Limpar tabela existente se houver problemas de schema\n",
    "print(\"🔄 Limpando banco de dados existente para evitar conflitos de schema...\")\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {nome_da_tabela_consolidada}\"))\n",
    "        conn.commit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"--- Iniciando o processo de ELT ---\")\n",
    "\n",
    "for i, nome_arquivo in enumerate(lista_arquivos_csv):\n",
    "    print(f\"Processando arquivo: {nome_arquivo}\")\n",
    "    \n",
    "    # Lendo os arquivos csv para um dataframe (EXTRACT)\n",
    "    try:\n",
    "        df_temp = pd.read_csv(nome_arquivo, sep=';', encoding='utf-8')\n",
    "        \n",
    "        # Adicionar uma coluna para saber a origem do dado\n",
    "        ano = nome_arquivo.replace('acidentes', '').replace('.csv', '').replace('data/', '')\n",
    "        df_temp['ano_do_dado'] = int(ano)\n",
    "        \n",
    "        # CARREGAMENTO (LOAD)\n",
    "        # Define a estratégia: 'replace' para o primeiro arquivo, 'append' para os demais.\n",
    "        if i == 0:\n",
    "            modo_de_insercao = 'replace'\n",
    "        else:\n",
    "            modo_de_insercao = 'append'\n",
    "\n",
    "        df_temp.to_sql(\n",
    "            nome_da_tabela_consolidada,\n",
    "            con=engine,\n",
    "            if_exists=modo_de_insercao,\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"- Arquivo {nome_arquivo} carregado com sucesso ({len(df_temp)} registros)\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRO: Arquivo {nome_arquivo} não encontrado. Detalhes: {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO: Problema ao processar {nome_arquivo}. Detalhes: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fazendo as transformações dentro do banco\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"\\n🔄 Iniciando transformações no banco de dados (ELT)...\")\n",
    "        \n",
    "        # Verificar se a tabela existe e tem dados\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {nome_da_tabela_consolidada}\"))\n",
    "        total_inicial = result.fetchone()[0]\n",
    "        print(f\"Total de registros carregados: {total_inicial}\")\n",
    "        \n",
    "        if total_inicial == 0:\n",
    "            print(\"❌ Nenhum dado foi carregado. Verifique os arquivos CSV.\")\n",
    "        else:\n",
    "            # Criar coluna TimeStamp\n",
    "            conn.execute(text(\"ALTER TABLE dados_brutos_sinistros ADD COLUMN TimeStamp TEXT;\"))\n",
    "            \n",
    "            # Tratando 2 dados específicos que foram mal preenchidos nos dados de 2019\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET hora = null\n",
    "                WHERE hora in ('48:00:00', '1049592:00:00');\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Preencher a coluna timestamp com DATA + hora\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET TimeStamp =\n",
    "                    coalesce(data, DATA) || ' ' ||\n",
    "                    CASE\n",
    "                        WHEN hora = '24:00:00' THEN '00:00:00'\n",
    "                        WHEN hora IS NULL THEN '00:00:00'\n",
    "                        ELSE hora\n",
    "                    END;\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Coluna 'timestamp' preenchida com DATA e hora.\")\n",
    "\n",
    "            # Tratar campo de número, transformando em -1\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET numero = CASE\n",
    "                    WHEN numero IS NULL OR TRIM(numero) = '' THEN -1\n",
    "                    ELSE numero\n",
    "                END;\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Endereço e número tratados.\")\n",
    "\n",
    "            colunas_numericas = ['auto', 'moto', 'ciclom', 'ciclista', 'pedestre',\n",
    "                                 'onibus', 'caminhao', 'viatura', 'outros',\n",
    "                                 'vitimas', 'vitimasfatais', 'num_semaforo']\n",
    "\n",
    "            # Padronizando colunas que são numéricas\n",
    "            for col in colunas_numericas:\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET {col} = CASE\n",
    "                        WHEN TRIM({col}) = '' OR {col} IS NULL THEN 0\n",
    "                        ELSE CAST({col} as INTEGER)\n",
    "                    END;\n",
    "                \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Colunas numéricas padronizadas.\")\n",
    "            \n",
    "            # Criando coluna de tipo do acidente\n",
    "            conn.execute(text(\"ALTER TABLE dados_brutos_sinistros ADD COLUMN tipo_acidente TEXT;\"))\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET tipo_acidente = CASE\n",
    "                    WHEN ciclista > 0 THEN 'Com ciclista'\n",
    "                    WHEN pedestre > 0 THEN 'Com pedestre'\n",
    "                    WHEN vitimasfatais > 0 THEN 'Fatal'\n",
    "                    ELSE 'Outros'\n",
    "                END;\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Coluna 'tipo_acidente' criada.\")\n",
    "\n",
    "            # Definindo que caso não haja informação sobre vítimas, colocar como 'SEM VÍTIMA'\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET natureza_acidente = 'SEM VÍTIMA'\n",
    "                WHERE natureza_acidente IS NULL;\n",
    "            \"\"\"))\n",
    "\n",
    "            # Tratamento de colunas de texto\n",
    "            colunas_strings = ['bairro', 'endereco', 'detalhe_endereco_acidente', 'complemento', \n",
    "                              'endereco_cruzamento', 'conservacao_via', 'ponto_controle', 'situacao_placa',\n",
    "                              'divisao_via1', 'divisao_via2', 'divisao_via3', 'bairro_cruzamento', 'tipo',\n",
    "                              'acidente_verificado', 'tempo_clima', 'situacao_semaforo', 'sinalizacao',\n",
    "                              'sentido_via', 'condicao_via', 'mao_direcao']\n",
    "            \n",
    "            for col in colunas_strings:\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET {col} = COALESCE(NULLIF(TRIM({col}), ''), 'Desconhecido');\n",
    "                \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Colunas de texto padronizadas.\")\n",
    "\n",
    "            # Adicionar coluna 'id_linha'\n",
    "            conn.execute(text(\"ALTER TABLE dados_brutos_sinistros ADD COLUMN id_linha INTEGER;\"))\n",
    "            conn.execute(text(\"UPDATE dados_brutos_sinistros SET id_linha = ROWID;\"))\n",
    "            conn.commit()\n",
    "            print(\"- Coluna 'id_linha' adicionada e preenchida.\")\n",
    "\n",
    "            # Tratamento da velocidade máxima da via (se a coluna existir)\n",
    "            try:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET velocidade_max_via = CASE\n",
    "                        WHEN velocidade_max_via LIKE '%km/h%' THEN TRIM(REPLACE(velocidade_max_via, 'km/h', ''))\n",
    "                        ELSE velocidade_max_via\n",
    "                    END;\n",
    "                \"\"\"))\n",
    "                \n",
    "                conn.execute(text(\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET velocidade_max_via = CASE\n",
    "                        WHEN TRIM(velocidade_max_via) GLOB '[0-9]*' THEN CAST(velocidade_max_via AS INTEGER)\n",
    "                        ELSE -1\n",
    "                    END;\n",
    "                \"\"\"))\n",
    "                conn.commit()\n",
    "                print(\"- Velocidade máxima da via tratada.\")\n",
    "            except Exception as e:\n",
    "                print(f\"- Velocidade máxima da via não encontrada ou erro: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante as transformações: {e}\")\n",
    "\n",
    "# Verificação Final\n",
    "print(\"\\n--- Verificação Final do Processo ELT ---\")\n",
    "\n",
    "try:\n",
    "    # Contar o total de registros na tabela consolidada\n",
    "    total_registros = pd.read_sql(f\"SELECT COUNT(*) FROM {nome_da_tabela_consolidada}\", engine).iloc[0,0]\n",
    "    print(f\"A tabela final '{nome_da_tabela_consolidada}' contém um total de {total_registros} registros.\")\n",
    "\n",
    "    # Agrupar por ano para confirmar que os dados dos 3 arquivos foram carregados\n",
    "    print(\"\\nContagem de registros por ano de origem do dado:\")\n",
    "    df_verificacao = pd.read_sql(f'SELECT ano_do_dado, COUNT(*) as total_de_linhas FROM {nome_da_tabela_consolidada} GROUP BY ano_do_dado', engine)\n",
    "    print(df_verificacao)\n",
    "\n",
    "    # Selecionar todos os dados\n",
    "    df_all = pd.read_sql(f'SELECT * FROM {nome_da_tabela_consolidada}', engine)\n",
    "    print(f\"\\nDataFrame final criado com {len(df_all)} registros e {len(df_all.columns)} colunas.\")\n",
    "    print(\"\\nPrimeiras 5 linhas:\")\n",
    "    print(df_all.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro na verificação final: {e}\")\n",
    "    print(\"Verifique se os dados foram carregados corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18794,
     "status": "ok",
     "timestamp": 1754516990947,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "wwp0pArBlJjp",
    "outputId": "2c4fcd9f-eb83-45b1-85d6-403f95a7cb9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando o arquivo Excel: dados_brutos_sinistros_consolidados.xlsx...\n",
      "✅ Arquivo 'dados_brutos_sinistros_consolidados.xlsx' gerado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# --- CÓDIGO PARA GERAR O ARQUIVO XLSX ---\n",
    "# (Execute esta célula após ter executado seu código que cria o DataFrame 'df_all')\n",
    "\n",
    "# Nome que você deseja para o arquivo Excel de saída\n",
    "nome_do_arquivo_saida = 'dados_brutos_sinistros_consolidados.xlsx'\n",
    "\n",
    "print(f\"Gerando o arquivo Excel: {nome_do_arquivo_saida}...\")\n",
    "\n",
    "df_all.to_excel(\n",
    "    nome_do_arquivo_saida,\n",
    "    sheet_name='Dados de Sinistros',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Arquivo '{nome_do_arquivo_saida}' gerado com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
