{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuqGfgwfrwl1"
   },
   "source": [
    "# üìä Pipeline de Dados - Acidentes de Tr√¢nsito (2019-2021)\n",
    "\n",
    "## üõ†Ô∏è Imports e Configura√ß√µes Iniciais\n",
    "\n",
    "Este notebook implementa pipelines ETL e ELT para integra√ß√£o de dados de acidentes de tr√¢nsito de tr√™s anos consecutivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4665,
     "status": "ok",
     "timestamp": 1754516960194,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "lh1NwZWvrvnj",
    "outputId": "384102db-5596-46b5-d60f-1f69e421c9d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "619.88s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/ec/d3/3c37cb724d76a841f14b8f5fe57e5e3645207cc67370e4f84717e8bb7657/pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b7/13/e792d7209261afb0c9f4759ffef6135b35c77c6349a151f488f531d13595/numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b7/13/e792d7209261afb0c9f4759ffef6135b35c77c6349a151f488f531d13595/numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sqlalchemy\n",
      "  Obtaining dependency information for sqlalchemy from https://files.pythonhosted.org/packages/66/17/19be542fe9dd64a766090e90e789e86bdaa608affda6b3c1e118a25a2509/sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting sqlalchemy\n",
      "  Obtaining dependency information for sqlalchemy from https://files.pythonhosted.org/packages/66/17/19be542fe9dd64a766090e90e789e86bdaa608affda6b3c1e118a25a2509/sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting openpyxl\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl.metadata\n",
      "Collecting openpyxl\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl.metadata\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sqlalchemy) (4.14.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sqlalchemy) (4.14.1)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/c1/8b/5fe2cc11fee489817272089c4203e679c63b570a5aaeb18d852ae3cbba6a/et_xmlfile-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/c1/8b/5fe2cc11fee489817272089c4203e679c63b570a5aaeb18d852ae3cbba6a/et_xmlfile-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/esdrasalbino/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/10.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/5.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/2.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mDownloading sqlalchemy-2.0.42-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/250.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: pytz, tzdata, sqlalchemy, numpy, et-xmlfile, pandas, openpyxl\n",
      "Installing collected packages: pytz, tzdata, sqlalchemy, numpy, et-xmlfile, pandas, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 numpy-2.3.2 openpyxl-3.1.5 pandas-2.3.1 pytz-2025.2 sqlalchemy-2.0.42 tzdata-2025.2\n",
      "Successfully installed et-xmlfile-2.0.0 numpy-2.3.2 openpyxl-3.1.5 pandas-2.3.1 pytz-2025.2 sqlalchemy-2.0.42 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Todas as bibliotecas foram importadas com sucesso!\n",
      "üì¶ Pandas vers√£o: 2.3.1\n",
      "üì¶ SQLAlchemy vers√£o: 2.0.42\n",
      "‚úÖ Todas as bibliotecas foram importadas com sucesso!\n",
      "üì¶ Pandas vers√£o: 2.3.1\n",
      "üì¶ SQLAlchemy vers√£o: 2.0.42\n"
     ]
    }
   ],
   "source": [
    "# Instalar pacotes necess√°rios no ambiente do notebook\n",
    "%pip install pandas numpy sqlalchemy openpyxl\n",
    "\n",
    "# Bibliotecas para manipula√ß√£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para banco de dados\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"üì¶ Pandas vers√£o: {pd.__version__}\")\n",
    "print(f\"üì¶ SQLAlchemy vers√£o: {sqlalchemy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2041,
     "status": "ok",
     "timestamp": 1754516962236,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "AaSDDO64KtzF",
    "outputId": "95ec8ec2-dfe1-4a5f-8bcf-6bfe25a8a4e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o processo de ETL ---\n",
      "Etapa 1: Extraindo dados dos arquivos CSV...\n",
      "Dados extra√≠dos com sucesso!\n",
      "\n",
      "Etapa 2: Transformando os dados...\n",
      "- Nomes de colunas padronizados.\n",
      "- Colunas harmonizadas entre os datasets.\n",
      "- Colunas de data e hora convertidas para o formato timestamp.\n",
      "- Coluna 'ano' criada.\n",
      "- Tipos de dados das colunas num√©ricas padronizados para inteiro.\n",
      "- Endere√ßos padronizados.\n",
      "\n",
      "Etapa 3: Carregando os dados transformados...\n",
      "- DataFrames unificados com sucesso.\n",
      "\n",
      "--- Processo de ETL Conclu√≠do! ---\n",
      "A base de dados unificada foi salva como: 'acidentes_unificados_2019-2021.csv'\n",
      "Total de registros na base unificada: 18534\n",
      "\n",
      "Amostra dos 5 primeiros registros da base final:\n",
      "    ano           timestamp natureza_acidente    situacao       bairro  \\\n",
      "0  2019 2019-01-01 00:41:00        SEM V√çTIMA  FINALIZADA        IPSEP   \n",
      "1  2019 2019-01-01 01:37:00        SEM V√çTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019 2019-01-01 14:20:00        SEM V√çTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019 2019-01-01 02:53:00        SEM V√çTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019 2019-01-01 08:17:00        COM V√çTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco  numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE      -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA      -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA      -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR     100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS      63                 Desconhecido   \n",
      "\n",
      "                                         complemento bairro_cruzamento  ...  \\\n",
      "0                             LADO OPOSTO AO N¬∫ 3257             IPSEP  ...   \n",
      "1                                       Desconhecido        BOA VIAGEM  ...   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...        BOA VIAGEM  ...   \n",
      "3                     EM FRENTE A ART LED ILUMINA√á√ÉO       IMBIRIBEIRA  ...   \n",
      "4                            ED. JARDINS DA JAQUEIRA          JAQUEIRA  ...   \n",
      "\n",
      "       sinalizacao condicao_via  conservacao_via     ponto_controle  \\\n",
      "0  Perfeito estado         Seca  Perfeito estado         N√£o existe   \n",
      "1  Perfeito estado         Seca  Perfeito estado  Faixa de pedestre   \n",
      "2              NaN          NaN              NaN                NaN   \n",
      "3              NaN          NaN              NaN                NaN   \n",
      "4  Perfeito estado         Seca  Perfeito estado         N√£o existe   \n",
      "\n",
      "   situacao_placa  velocidade_max_via  mao_direcao      divisao_via1  \\\n",
      "0   N√£o h√° placas             60 km/h        √önica  Faixa seccionada   \n",
      "1   N√£o h√° placas                 NaN        √önica        N√£o existe   \n",
      "2             NaN                 NaN          NaN               NaN   \n",
      "3             NaN                 NaN          NaN               NaN   \n",
      "4   N√£o h√° placas             40 km/h        √önica  Faixa seccionada   \n",
      "\n",
      "   divisao_via2  divisao_via3  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "Informa√ß√µes da base final:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18534 entries, 0 to 18533\n",
      "Data columns (total 37 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   ano                        18534 non-null  int64         \n",
      " 1   timestamp                  18516 non-null  datetime64[ns]\n",
      " 2   natureza_acidente          18529 non-null  object        \n",
      " 3   situacao                   18529 non-null  object        \n",
      " 4   bairro                     18347 non-null  object        \n",
      " 5   endereco                   18534 non-null  object        \n",
      " 6   numero                     18534 non-null  int64         \n",
      " 7   detalhe_endereco_acidente  18534 non-null  object        \n",
      " 8   complemento                18534 non-null  object        \n",
      " 9   bairro_cruzamento          18341 non-null  object        \n",
      " 10  num_semaforo               18534 non-null  int64         \n",
      " 11  sentido_via                12651 non-null  object        \n",
      " 12  tipo                       18476 non-null  object        \n",
      " 13  auto                       18534 non-null  int64         \n",
      " 14  moto                       18534 non-null  int64         \n",
      " 15  ciclom                     18534 non-null  int64         \n",
      " 16  ciclista                   18534 non-null  int64         \n",
      " 17  pedestre                   18534 non-null  int64         \n",
      " 18  onibus                     18534 non-null  int64         \n",
      " 19  caminhao                   18534 non-null  int64         \n",
      " 20  viatura                    18534 non-null  int64         \n",
      " 21  outros                     18534 non-null  int64         \n",
      " 22  vitimas                    18534 non-null  int64         \n",
      " 23  vitimasfatais              18534 non-null  int64         \n",
      " 24  acidente_verificado        14266 non-null  object        \n",
      " 25  tempo_clima                14879 non-null  object        \n",
      " 26  situacao_semaforo          14686 non-null  object        \n",
      " 27  sinalizacao                14589 non-null  object        \n",
      " 28  condicao_via               14830 non-null  object        \n",
      " 29  conservacao_via            14606 non-null  object        \n",
      " 30  ponto_controle             13516 non-null  object        \n",
      " 31  situacao_placa             13455 non-null  object        \n",
      " 32  velocidade_max_via         4609 non-null   object        \n",
      " 33  mao_direcao                14569 non-null  object        \n",
      " 34  divisao_via1               14043 non-null  object        \n",
      " 35  divisao_via2               1449 non-null   object        \n",
      " 36  divisao_via3               248 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(14), object(22)\n",
      "memory usage: 5.2+ MB\n",
      "\n",
      "Etapa 3.3: Inserindo os dados no banco de dados...\n",
      "\n",
      "--- Processo de ETL Conclu√≠do! ---\n",
      "A base de dados unificada foi salva como: 'acidentes_unificados_2019-2021.csv'\n",
      "Total de registros na base unificada: 18534\n",
      "\n",
      "Amostra dos 5 primeiros registros da base final:\n",
      "    ano           timestamp natureza_acidente    situacao       bairro  \\\n",
      "0  2019 2019-01-01 00:41:00        SEM V√çTIMA  FINALIZADA        IPSEP   \n",
      "1  2019 2019-01-01 01:37:00        SEM V√çTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019 2019-01-01 14:20:00        SEM V√çTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019 2019-01-01 02:53:00        SEM V√çTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019 2019-01-01 08:17:00        COM V√çTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco  numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE      -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA      -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA      -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR     100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS      63                 Desconhecido   \n",
      "\n",
      "                                         complemento bairro_cruzamento  ...  \\\n",
      "0                             LADO OPOSTO AO N¬∫ 3257             IPSEP  ...   \n",
      "1                                       Desconhecido        BOA VIAGEM  ...   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...        BOA VIAGEM  ...   \n",
      "3                     EM FRENTE A ART LED ILUMINA√á√ÉO       IMBIRIBEIRA  ...   \n",
      "4                            ED. JARDINS DA JAQUEIRA          JAQUEIRA  ...   \n",
      "\n",
      "       sinalizacao condicao_via  conservacao_via     ponto_controle  \\\n",
      "0  Perfeito estado         Seca  Perfeito estado         N√£o existe   \n",
      "1  Perfeito estado         Seca  Perfeito estado  Faixa de pedestre   \n",
      "2              NaN          NaN              NaN                NaN   \n",
      "3              NaN          NaN              NaN                NaN   \n",
      "4  Perfeito estado         Seca  Perfeito estado         N√£o existe   \n",
      "\n",
      "   situacao_placa  velocidade_max_via  mao_direcao      divisao_via1  \\\n",
      "0   N√£o h√° placas             60 km/h        √önica  Faixa seccionada   \n",
      "1   N√£o h√° placas                 NaN        √önica        N√£o existe   \n",
      "2             NaN                 NaN          NaN               NaN   \n",
      "3             NaN                 NaN          NaN               NaN   \n",
      "4   N√£o h√° placas             40 km/h        √önica  Faixa seccionada   \n",
      "\n",
      "   divisao_via2  divisao_via3  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "\n",
      "Informa√ß√µes da base final:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18534 entries, 0 to 18533\n",
      "Data columns (total 37 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   ano                        18534 non-null  int64         \n",
      " 1   timestamp                  18516 non-null  datetime64[ns]\n",
      " 2   natureza_acidente          18529 non-null  object        \n",
      " 3   situacao                   18529 non-null  object        \n",
      " 4   bairro                     18347 non-null  object        \n",
      " 5   endereco                   18534 non-null  object        \n",
      " 6   numero                     18534 non-null  int64         \n",
      " 7   detalhe_endereco_acidente  18534 non-null  object        \n",
      " 8   complemento                18534 non-null  object        \n",
      " 9   bairro_cruzamento          18341 non-null  object        \n",
      " 10  num_semaforo               18534 non-null  int64         \n",
      " 11  sentido_via                12651 non-null  object        \n",
      " 12  tipo                       18476 non-null  object        \n",
      " 13  auto                       18534 non-null  int64         \n",
      " 14  moto                       18534 non-null  int64         \n",
      " 15  ciclom                     18534 non-null  int64         \n",
      " 16  ciclista                   18534 non-null  int64         \n",
      " 17  pedestre                   18534 non-null  int64         \n",
      " 18  onibus                     18534 non-null  int64         \n",
      " 19  caminhao                   18534 non-null  int64         \n",
      " 20  viatura                    18534 non-null  int64         \n",
      " 21  outros                     18534 non-null  int64         \n",
      " 22  vitimas                    18534 non-null  int64         \n",
      " 23  vitimasfatais              18534 non-null  int64         \n",
      " 24  acidente_verificado        14266 non-null  object        \n",
      " 25  tempo_clima                14879 non-null  object        \n",
      " 26  situacao_semaforo          14686 non-null  object        \n",
      " 27  sinalizacao                14589 non-null  object        \n",
      " 28  condicao_via               14830 non-null  object        \n",
      " 29  conservacao_via            14606 non-null  object        \n",
      " 30  ponto_controle             13516 non-null  object        \n",
      " 31  situacao_placa             13455 non-null  object        \n",
      " 32  velocidade_max_via         4609 non-null   object        \n",
      " 33  mao_direcao                14569 non-null  object        \n",
      " 34  divisao_via1               14043 non-null  object        \n",
      " 35  divisao_via2               1449 non-null   object        \n",
      " 36  divisao_via3               248 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(14), object(22)\n",
      "memory usage: 5.2+ MB\n",
      "\n",
      "Etapa 3.3: Inserindo os dados no banco de dados...\n",
      "- Dados inseridos com sucesso na tabela 'acidentes' do banco SQLite (arquivo 'banco_etl.db').\n",
      "- Dados inseridos com sucesso na tabela 'acidentes' do banco SQLite (arquivo 'banco_etl.db').\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def run_etl_pipeline():\n",
    "    \"\"\"\n",
    "    Executa o pipeline completo de ETL para os dados de acidentes de 2019, 2020 e 2021.\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando o processo de ETL ---\")\n",
    "\n",
    "    # --- 1. EXTRA√á√ÉO (Extract) ---\n",
    "    # Carregando os tr√™s conjuntos de dados a partir dos arquivos CSV.\n",
    "    try:\n",
    "        print(\"Etapa 1: Extraindo dados dos arquivos CSV...\")\n",
    "        df_2019 = pd.read_csv('data/acidentes-2019.csv', delimiter=';')\n",
    "        df_2020 = pd.read_csv('data/acidentes_2020-novo.csv', delimiter=';')\n",
    "        df_2021 = pd.read_csv('data/acidentes2021.csv', delimiter=';')\n",
    "        print(\"Dados extra√≠dos com sucesso!\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRO: Arquivo n√£o encontrado. Verifique se os nomes dos arquivos est√£o corretos. Detalhes: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. TRANSFORMA√á√ÉO (Transform) ---\n",
    "    print(\"\\nEtapa 2: Transformando os dados...\")\n",
    "\n",
    "    # 2.1 Padroniza√ß√£o dos nomes das colunas\n",
    "    # O dataset de 2019 tem a coluna 'DATA' em mai√∫sculo. Vamos padronizar para 'data'.\n",
    "    df_2019.rename(columns={'DATA': 'data'}, inplace=True)\n",
    "    print(\"- Nomes de colunas padronizados.\")\n",
    "\n",
    "    # 2.2 Harmoniza√ß√£o das colunas (removendo colunas inconsistentes)\n",
    "    # Colunas presentes em 2019 mas n√£o em 2020/2021\n",
    "    cols_to_drop_2019 = ['endereco_cruzamento', 'numero_cruzamento', 'referencia_cruzamento', 'descricao']\n",
    "    df_2019.drop(columns=[col for col in cols_to_drop_2019 if col in df_2019.columns], inplace=True)\n",
    "\n",
    "    # Coluna 'descricao' presente em 2020 mas n√£o em 2021\n",
    "    if 'descricao' in df_2020.columns:\n",
    "        df_2020.drop(columns=['descricao'], inplace=True)\n",
    "\n",
    "    # A coluna 'tipo' em 2021 est√° no lugar de 'descricao', mas para unificar vamos manter apenas as colunas em comum.\n",
    "    # A base de 2021 n√£o tem a coluna 'descricao', ent√£o n√£o √© necess√°rio fazer nada.\n",
    "\n",
    "    print(\"- Colunas harmonizadas entre os datasets.\")\n",
    "\n",
    "    # Lista dos dataframes para aplicar as transforma√ß√µes em lote\n",
    "    dataframes = [df_2019, df_2020, df_2021]\n",
    "    ano_inicio = 2019 # Precisa ser alterado caso entre anos anteriores no data set\n",
    "\n",
    "    # 2.3 Transforma√ß√£o de data e hora\n",
    "    for i, df in enumerate(dataframes):\n",
    "        ano = ano_inicio + i\n",
    "        # Corrigindo valores de hora inv√°lidos como '24:00:00'\n",
    "        df['hora'] = df['hora'].str.replace('24:00:00', '00:00:00', regex=False)\n",
    "        # Combinando 'data' e 'hora' em uma √∫nica coluna 'timestamp'\n",
    "        # 'coerce' transforma datas inv√°lidas em NaT (Not a Time)\n",
    "        df['timestamp'] = pd.to_datetime(df['data'] + ' ' + df['hora'], errors='coerce')\n",
    "        # Adicionando a coluna 'ano'\n",
    "        df['ano'] = ano\n",
    "        # Removendo as colunas originais\n",
    "        df.drop(columns=['data', 'hora'], inplace=True)\n",
    "\n",
    "    print(\"- Colunas de data e hora convertidas para o formato timestamp.\")\n",
    "    print(\"- Coluna 'ano' criada.\")\n",
    "\n",
    "    # 2.4 Padroniza√ß√£o de tipos de dados num√©ricos\n",
    "    colunas_numericas = [\n",
    "        'auto', 'moto', 'ciclom', 'ciclista', 'pedestre', 'onibus',\n",
    "        'caminhao', 'viatura', 'outros', 'vitimas', 'vitimasfatais'\n",
    "    ]\n",
    "\n",
    "    for df in dataframes:\n",
    "        for col in colunas_numericas:\n",
    "            if col in df.columns:\n",
    "                # Converte para num√©rico, tratando erros, e preenche NaNs com 0\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "                # Converte para inteiro\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "    # Tratamento espec√≠fico para 'num_semaforo' que tem tipos diferentes\n",
    "    for df in dataframes:\n",
    "        if 'num_semaforo' in df.columns:\n",
    "            df['num_semaforo'] = pd.to_numeric(df['num_semaforo'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    print(\"- Tipos de dados das colunas num√©ricas padronizados para inteiro.\")\n",
    "\n",
    "    # Tratamento de endereco\n",
    "    for df in dataframes:\n",
    "        if 'endereco' in df.columns:\n",
    "            df['endereco'] = df['endereco'].str.strip().replace(r'^\\s*$', np.nan, regex=True).fillna('Desconhecido')\n",
    "        if 'detalhe_endereco_acidente' in df.columns:\n",
    "            df['detalhe_endereco_acidente'] = df['detalhe_endereco_acidente'].str.strip().replace(r'^\\s*$', np.nan, regex=True).fillna('Desconhecido')\n",
    "        if 'complemento' in df.columns:\n",
    "            df['complemento'] = df['complemento'].str.strip().replace(r'^\\s*$', np.nan, regex=True).fillna('Desconhecido')\n",
    "        if 'numero' in df.columns:\n",
    "            df['numero'] = pd.to_numeric(df['numero'], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "    print(\"- Endere√ßos padronizados.\")\n",
    "\n",
    "    # --- 3. CARGA (Load) ---\n",
    "    print(\"\\nEtapa 3: Carregando os dados transformados...\")\n",
    "\n",
    "    # 3.1 Unifica√ß√£o dos DataFrames\n",
    "    # Concatenando os tr√™s dataframes em um s√≥\n",
    "    df_unificado = pd.concat(dataframes, ignore_index=True)\n",
    "    print(\"- DataFrames unificados com sucesso.\")\n",
    "\n",
    "    # Reordenando colunas para colocar 'ano' e 'timestamp' no in√≠cio\n",
    "    cols = ['ano', 'timestamp'] + [col for col in df_unificado.columns if col not in ['ano', 'timestamp']]\n",
    "    df_unificado = df_unificado[cols]\n",
    "\n",
    "    # 3.2 Salvando o resultado em um novo arquivo CSV\n",
    "    output_filename = 'acidentes_unificados_2019-2021.csv'\n",
    "    df_unificado.to_csv(output_filename, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "    print(f\"\\n--- Processo de ETL Conclu√≠do! ---\")\n",
    "    print(f\"A base de dados unificada foi salva como: '{output_filename}'\")\n",
    "    print(f\"Total de registros na base unificada: {len(df_unificado)}\")\n",
    "    print(\"\\nAmostra dos 5 primeiros registros da base final:\")\n",
    "    print(df_unificado.head())\n",
    "    print(\"\\nInforma√ß√µes da base final:\")\n",
    "    df_unificado.info()\n",
    "\n",
    "    # 3.3 Inserindo no banco de dados\n",
    "    print(\"\\nEtapa 3.3: Inserindo os dados no banco de dados...\")\n",
    "\n",
    "    # criando um arquivo .db\n",
    "    nome_do_banco_de_dados2 = \"banco_etl.db\"\n",
    "    nome_tabela = 'acidentes'\n",
    "\n",
    "    engine = create_engine(f'sqlite:///{nome_do_banco_de_dados2}')\n",
    "\n",
    "    df_unificado.to_sql(nome_tabela,\n",
    "                        con=engine,\n",
    "                        if_exists='replace',\n",
    "                        index=False)\n",
    "\n",
    "    print(f\"- Dados inseridos com sucesso na tabela '{nome_tabela}' do banco SQLite (arquivo '{nome_do_banco_de_dados2}').\")\n",
    "# Executando a fun√ß√£o principal do pipeline\n",
    "run_etl_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4yTS38lrPrG"
   },
   "source": [
    "# ETAPA ELT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9914,
     "status": "ok",
     "timestamp": 1754516972151,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "Oc3Ko0KurZ78",
    "outputId": "45a21907-1246-4ab1-d495-cfa42d6d8dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Limpando banco de dados existente para evitar conflitos de schema...\n",
      "--- Iniciando o processo de ELT ---\n",
      "Processando arquivo: data/acidentes2019.csv\n",
      "--- Iniciando o processo de ELT ---\n",
      "Processando arquivo: data/acidentes2019.csv\n",
      "- Arquivo data/acidentes2019.csv carregado com sucesso (12062 registros)\n",
      "Processando arquivo: data/acidentes2020.csv\n",
      "- Arquivo data/acidentes2020.csv carregado com sucesso (4092 registros)\n",
      "Processando arquivo: data/acidentes2021.csv\n",
      "- Arquivo data/acidentes2021.csv carregado com sucesso (2380 registros)\n",
      "\n",
      "üîÑ Iniciando transforma√ß√µes no banco de dados (ELT)...\n",
      "Total de registros carregados: 18534\n",
      "- Arquivo data/acidentes2019.csv carregado com sucesso (12062 registros)\n",
      "Processando arquivo: data/acidentes2020.csv\n",
      "- Arquivo data/acidentes2020.csv carregado com sucesso (4092 registros)\n",
      "Processando arquivo: data/acidentes2021.csv\n",
      "- Arquivo data/acidentes2021.csv carregado com sucesso (2380 registros)\n",
      "\n",
      "üîÑ Iniciando transforma√ß√µes no banco de dados (ELT)...\n",
      "Total de registros carregados: 18534\n",
      "- Coluna 'timestamp' preenchida com DATA e hora.\n",
      "- Endere√ßo e n√∫mero tratados.\n",
      "- Coluna 'timestamp' preenchida com DATA e hora.\n",
      "- Endere√ßo e n√∫mero tratados.\n",
      "- Colunas num√©ricas padronizadas.\n",
      "- Coluna 'tipo_acidente' criada.\n",
      "- Colunas num√©ricas padronizadas.\n",
      "- Coluna 'tipo_acidente' criada.\n",
      "- Colunas de texto padronizadas.\n",
      "- Coluna 'id_linha' adicionada e preenchida.\n",
      "- Colunas de texto padronizadas.\n",
      "- Coluna 'id_linha' adicionada e preenchida.\n",
      "- Velocidade m√°xima da via tratada.\n",
      "\n",
      "--- Verifica√ß√£o Final do Processo ELT ---\n",
      "A tabela final 'dados_brutos_sinistros' cont√©m um total de 18534 registros.\n",
      "\n",
      "Contagem de registros por ano de origem do dado:\n",
      "   ano_do_dado  total_de_linhas\n",
      "0         2019            12062\n",
      "1         2020             4092\n",
      "2         2021             2380\n",
      "\n",
      "DataFrame final criado com 18534 registros e 45 colunas.\n",
      "\n",
      "Primeiras 5 linhas:\n",
      "         DATA      hora natureza_acidente    situacao       bairro  \\\n",
      "0  2019-01-01  00:41:00        SEM V√çTIMA  FINALIZADA        IPSEP   \n",
      "1  2019-01-01  01:37:00        SEM V√çTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019-01-01  14:20:00        SEM V√çTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019-01-01  02:53:00        SEM V√çTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019-01-01  08:17:00        COM V√çTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE     -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA     -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA     -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR    100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS     63                 Desconhecido   \n",
      "\n",
      "                                         complemento  \\\n",
      "0                             LADO OPOSTO AO N¬∫ 3257   \n",
      "1                                       Desconhecido   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...   \n",
      "3                     EM FRENTE A ART LED ILUMINA√á√ÉO   \n",
      "4                            ED. JARDINS DA JAQUEIRA   \n",
      "\n",
      "               endereco_cruzamento  ... situacao_placa velocidade_max_via  \\\n",
      "0                        AV RECIFE  ...  N√£o h√° placas                 60   \n",
      "1       RUA PADRE BERNADINO PESSOA  ...  N√£o h√° placas                 -1   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA  ...   Desconhecido                 -1   \n",
      "3            AV GENERAL MAC ARTHUR  ...   Desconhecido                 -1   \n",
      "4                   RUA TITO ROSAS  ...  N√£o h√° placas                 40   \n",
      "\n",
      "    mao_direcao      divisao_via1  divisao_via2  divisao_via3 ano_do_dado  \\\n",
      "0         √önica  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "1         √önica        N√£o existe  Desconhecido  Desconhecido        2019   \n",
      "2  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "3  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "4         √önica  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "\n",
      "             TimeStamp  tipo_acidente  id_linha  \n",
      "0  2019-01-01 00:41:00         Outros         1  \n",
      "1  2019-01-01 01:37:00         Outros         2  \n",
      "2  2019-01-01 14:20:00         Outros         3  \n",
      "3  2019-01-01 02:53:00         Outros         4  \n",
      "4  2019-01-01 08:17:00   Com ciclista         5  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "- Velocidade m√°xima da via tratada.\n",
      "\n",
      "--- Verifica√ß√£o Final do Processo ELT ---\n",
      "A tabela final 'dados_brutos_sinistros' cont√©m um total de 18534 registros.\n",
      "\n",
      "Contagem de registros por ano de origem do dado:\n",
      "   ano_do_dado  total_de_linhas\n",
      "0         2019            12062\n",
      "1         2020             4092\n",
      "2         2021             2380\n",
      "\n",
      "DataFrame final criado com 18534 registros e 45 colunas.\n",
      "\n",
      "Primeiras 5 linhas:\n",
      "         DATA      hora natureza_acidente    situacao       bairro  \\\n",
      "0  2019-01-01  00:41:00        SEM V√çTIMA  FINALIZADA        IPSEP   \n",
      "1  2019-01-01  01:37:00        SEM V√çTIMA  FINALIZADA   BOA VIAGEM   \n",
      "2  2019-01-01  14:20:00        SEM V√çTIMA   CANCELADA   BOA VIAGEM   \n",
      "3  2019-01-01  02:53:00        SEM V√çTIMA   CANCELADA  IMBIRIBEIRA   \n",
      "4  2019-01-01  08:17:00        COM V√çTIMA  FINALIZADA     JAQUEIRA   \n",
      "\n",
      "                          endereco numero    detalhe_endereco_acidente  \\\n",
      "0                        AV RECIFE     -1                 Desconhecido   \n",
      "1       RUA PADRE BERNADINO PESSOA     -1  RUA MINISTRO NELSON HUNGRIA   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA     -1           RUA DOM JOSE LOPES   \n",
      "3            AV GENERAL MAC ARTHUR    100                     RUA JACY   \n",
      "4                   RUA TITO ROSAS     63                 Desconhecido   \n",
      "\n",
      "                                         complemento  \\\n",
      "0                             LADO OPOSTO AO N¬∫ 3257   \n",
      "1                                       Desconhecido   \n",
      "2  EM FRENTE A DELEGACIA DE BOA VIAGEM, LADO ESQU...   \n",
      "3                     EM FRENTE A ART LED ILUMINA√á√ÉO   \n",
      "4                            ED. JARDINS DA JAQUEIRA   \n",
      "\n",
      "               endereco_cruzamento  ... situacao_placa velocidade_max_via  \\\n",
      "0                        AV RECIFE  ...  N√£o h√° placas                 60   \n",
      "1       RUA PADRE BERNADINO PESSOA  ...  N√£o h√° placas                 -1   \n",
      "2  AV ENGENHEIRO DOMINGOS FERREIRA  ...   Desconhecido                 -1   \n",
      "3            AV GENERAL MAC ARTHUR  ...   Desconhecido                 -1   \n",
      "4                   RUA TITO ROSAS  ...  N√£o h√° placas                 40   \n",
      "\n",
      "    mao_direcao      divisao_via1  divisao_via2  divisao_via3 ano_do_dado  \\\n",
      "0         √önica  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "1         √önica        N√£o existe  Desconhecido  Desconhecido        2019   \n",
      "2  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "3  Desconhecido      Desconhecido  Desconhecido  Desconhecido        2019   \n",
      "4         √önica  Faixa seccionada  Desconhecido  Desconhecido        2019   \n",
      "\n",
      "             TimeStamp  tipo_acidente  id_linha  \n",
      "0  2019-01-01 00:41:00         Outros         1  \n",
      "1  2019-01-01 01:37:00         Outros         2  \n",
      "2  2019-01-01 14:20:00         Outros         3  \n",
      "3  2019-01-01 02:53:00         Outros         4  \n",
      "4  2019-01-01 08:17:00   Com ciclista         5  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "#INICIANDO A SUBIDA DOS DADOS BRUTOS DE 2019 AT√â 2021, Utilizando as bibliotecas panda e sqlalchemy\n",
    "\n",
    "# --- CONFIGURA√á√ÉO---\n",
    "# Nome dos arquivos CSV corrigidos para corresponder aos arquivos reais\n",
    "lista_arquivos_csv = ['data/acidentes2019.csv', 'data/acidentes2020.csv', 'data/acidentes2021.csv']\n",
    "\n",
    "# Nome do arquivo do banco de dados\n",
    "nome_do_banco_de_dados = 'banco_elt.db'\n",
    "\n",
    "# Nome da tabela onde os dados brutos ser√£o guardados\n",
    "nome_da_tabela_consolidada = 'dados_brutos_sinistros'\n",
    "\n",
    "# Conectar ao banco de dados\n",
    "engine = create_engine(f'sqlite:///{nome_do_banco_de_dados}')\n",
    "\n",
    "# Limpar tabela existente se houver problemas de schema\n",
    "print(\"üîÑ Limpando banco de dados existente para evitar conflitos de schema...\")\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {nome_da_tabela_consolidada}\"))\n",
    "        conn.commit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"--- Iniciando o processo de ELT ---\")\n",
    "\n",
    "for i, nome_arquivo in enumerate(lista_arquivos_csv):\n",
    "    print(f\"Processando arquivo: {nome_arquivo}\")\n",
    "    \n",
    "    # Lendo os arquivos csv para um dataframe (EXTRACT)\n",
    "    try:\n",
    "        df_temp = pd.read_csv(nome_arquivo, sep=';', encoding='utf-8')\n",
    "        \n",
    "        # Adicionar uma coluna para saber a origem do dado\n",
    "        ano = nome_arquivo.replace('acidentes', '').replace('.csv', '').replace('data/', '')\n",
    "        df_temp['ano_do_dado'] = int(ano)\n",
    "        \n",
    "        # CARREGAMENTO (LOAD)\n",
    "        # Define a estrat√©gia: 'replace' para o primeiro arquivo, 'append' para os demais.\n",
    "        if i == 0:\n",
    "            modo_de_insercao = 'replace'\n",
    "        else:\n",
    "            modo_de_insercao = 'append'\n",
    "\n",
    "        df_temp.to_sql(\n",
    "            nome_da_tabela_consolidada,\n",
    "            con=engine,\n",
    "            if_exists=modo_de_insercao,\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"- Arquivo {nome_arquivo} carregado com sucesso ({len(df_temp)} registros)\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRO: Arquivo {nome_arquivo} n√£o encontrado. Detalhes: {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO: Problema ao processar {nome_arquivo}. Detalhes: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fazendo as transforma√ß√µes dentro do banco\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"\\nüîÑ Iniciando transforma√ß√µes no banco de dados (ELT)...\")\n",
    "        \n",
    "        # Verificar se a tabela existe e tem dados\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {nome_da_tabela_consolidada}\"))\n",
    "        total_inicial = result.fetchone()[0]\n",
    "        print(f\"Total de registros carregados: {total_inicial}\")\n",
    "        \n",
    "        if total_inicial == 0:\n",
    "            print(\"‚ùå Nenhum dado foi carregado. Verifique os arquivos CSV.\")\n",
    "        else:\n",
    "            # Criar coluna TimeStamp\n",
    "            conn.execute(text(\"ALTER TABLE dados_brutos_sinistros ADD COLUMN TimeStamp TEXT;\"))\n",
    "            \n",
    "            # Tratando 2 dados espec√≠ficos que foram mal preenchidos nos dados de 2019\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET hora = null\n",
    "                WHERE hora in ('48:00:00', '1049592:00:00');\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Preencher a coluna timestamp com DATA + hora\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET TimeStamp =\n",
    "                    coalesce(data, DATA) || ' ' ||\n",
    "                    CASE\n",
    "                        WHEN hora = '24:00:00' THEN '00:00:00'\n",
    "                        WHEN hora IS NULL THEN '00:00:00'\n",
    "                        ELSE hora\n",
    "                    END;\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Coluna 'timestamp' preenchida com DATA e hora.\")\n",
    "\n",
    "            # Tratar campo de n√∫mero, transformando em -1\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET numero = CASE\n",
    "                    WHEN numero IS NULL OR TRIM(numero) = '' THEN -1\n",
    "                    ELSE numero\n",
    "                END;\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Endere√ßo e n√∫mero tratados.\")\n",
    "\n",
    "            colunas_numericas = ['auto', 'moto', 'ciclom', 'ciclista', 'pedestre',\n",
    "                                 'onibus', 'caminhao', 'viatura', 'outros',\n",
    "                                 'vitimas', 'vitimasfatais', 'num_semaforo']\n",
    "\n",
    "            # Padronizando colunas que s√£o num√©ricas\n",
    "            for col in colunas_numericas:\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET {col} = CASE\n",
    "                        WHEN TRIM({col}) = '' OR {col} IS NULL THEN 0\n",
    "                        ELSE CAST({col} as INTEGER)\n",
    "                    END;\n",
    "                \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Colunas num√©ricas padronizadas.\")\n",
    "            \n",
    "            # Criando coluna de tipo do acidente\n",
    "            conn.execute(text(\"ALTER TABLE dados_brutos_sinistros ADD COLUMN tipo_acidente TEXT;\"))\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET tipo_acidente = CASE\n",
    "                    WHEN ciclista > 0 THEN 'Com ciclista'\n",
    "                    WHEN pedestre > 0 THEN 'Com pedestre'\n",
    "                    WHEN vitimasfatais > 0 THEN 'Fatal'\n",
    "                    ELSE 'Outros'\n",
    "                END;\n",
    "            \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Coluna 'tipo_acidente' criada.\")\n",
    "\n",
    "            # Definindo que caso n√£o haja informa√ß√£o sobre v√≠timas, colocar como 'SEM V√çTIMA'\n",
    "            conn.execute(text(\"\"\"\n",
    "                UPDATE dados_brutos_sinistros\n",
    "                SET natureza_acidente = 'SEM V√çTIMA'\n",
    "                WHERE natureza_acidente IS NULL;\n",
    "            \"\"\"))\n",
    "\n",
    "            # Tratamento de colunas de texto\n",
    "            colunas_strings = ['bairro', 'endereco', 'detalhe_endereco_acidente', 'complemento', \n",
    "                              'endereco_cruzamento', 'conservacao_via', 'ponto_controle', 'situacao_placa',\n",
    "                              'divisao_via1', 'divisao_via2', 'divisao_via3', 'bairro_cruzamento', 'tipo',\n",
    "                              'acidente_verificado', 'tempo_clima', 'situacao_semaforo', 'sinalizacao',\n",
    "                              'sentido_via', 'condicao_via', 'mao_direcao']\n",
    "            \n",
    "            for col in colunas_strings:\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET {col} = COALESCE(NULLIF(TRIM({col}), ''), 'Desconhecido');\n",
    "                \"\"\"))\n",
    "            conn.commit()\n",
    "            print(\"- Colunas de texto padronizadas.\")\n",
    "\n",
    "            # Adicionar coluna 'id_linha'\n",
    "            conn.execute(text(\"ALTER TABLE dados_brutos_sinistros ADD COLUMN id_linha INTEGER;\"))\n",
    "            conn.execute(text(\"UPDATE dados_brutos_sinistros SET id_linha = ROWID;\"))\n",
    "            conn.commit()\n",
    "            print(\"- Coluna 'id_linha' adicionada e preenchida.\")\n",
    "\n",
    "            # Tratamento da velocidade m√°xima da via (se a coluna existir)\n",
    "            try:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET velocidade_max_via = CASE\n",
    "                        WHEN velocidade_max_via LIKE '%km/h%' THEN TRIM(REPLACE(velocidade_max_via, 'km/h', ''))\n",
    "                        ELSE velocidade_max_via\n",
    "                    END;\n",
    "                \"\"\"))\n",
    "                \n",
    "                conn.execute(text(\"\"\"\n",
    "                    UPDATE dados_brutos_sinistros\n",
    "                    SET velocidade_max_via = CASE\n",
    "                        WHEN TRIM(velocidade_max_via) GLOB '[0-9]*' THEN CAST(velocidade_max_via AS INTEGER)\n",
    "                        ELSE -1\n",
    "                    END;\n",
    "                \"\"\"))\n",
    "                conn.commit()\n",
    "                print(\"- Velocidade m√°xima da via tratada.\")\n",
    "            except Exception as e:\n",
    "                print(f\"- Velocidade m√°xima da via n√£o encontrada ou erro: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante as transforma√ß√µes: {e}\")\n",
    "\n",
    "# Verifica√ß√£o Final\n",
    "print(\"\\n--- Verifica√ß√£o Final do Processo ELT ---\")\n",
    "\n",
    "try:\n",
    "    # Contar o total de registros na tabela consolidada\n",
    "    total_registros = pd.read_sql(f\"SELECT COUNT(*) FROM {nome_da_tabela_consolidada}\", engine).iloc[0,0]\n",
    "    print(f\"A tabela final '{nome_da_tabela_consolidada}' cont√©m um total de {total_registros} registros.\")\n",
    "\n",
    "    # Agrupar por ano para confirmar que os dados dos 3 arquivos foram carregados\n",
    "    print(\"\\nContagem de registros por ano de origem do dado:\")\n",
    "    df_verificacao = pd.read_sql(f'SELECT ano_do_dado, COUNT(*) as total_de_linhas FROM {nome_da_tabela_consolidada} GROUP BY ano_do_dado', engine)\n",
    "    print(df_verificacao)\n",
    "\n",
    "    # Selecionar todos os dados\n",
    "    df_all = pd.read_sql(f'SELECT * FROM {nome_da_tabela_consolidada}', engine)\n",
    "    print(f\"\\nDataFrame final criado com {len(df_all)} registros e {len(df_all.columns)} colunas.\")\n",
    "    print(\"\\nPrimeiras 5 linhas:\")\n",
    "    print(df_all.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro na verifica√ß√£o final: {e}\")\n",
    "    print(\"Verifique se os dados foram carregados corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18794,
     "status": "ok",
     "timestamp": 1754516990947,
     "user": {
      "displayName": "Guilherme Campelo",
      "userId": "02124395282369883898"
     },
     "user_tz": 180
    },
    "id": "wwp0pArBlJjp",
    "outputId": "2c4fcd9f-eb83-45b1-85d6-403f95a7cb9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando o arquivo Excel: dados_brutos_sinistros_consolidados.xlsx...\n",
      "‚úÖ Arquivo 'dados_brutos_sinistros_consolidados.xlsx' gerado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# --- C√ìDIGO PARA GERAR O ARQUIVO XLSX ---\n",
    "# (Execute esta c√©lula ap√≥s ter executado seu c√≥digo que cria o DataFrame 'df_all')\n",
    "\n",
    "# Nome que voc√™ deseja para o arquivo Excel de sa√≠da\n",
    "nome_do_arquivo_saida = 'dados_brutos_sinistros_consolidados.xlsx'\n",
    "\n",
    "print(f\"Gerando o arquivo Excel: {nome_do_arquivo_saida}...\")\n",
    "\n",
    "df_all.to_excel(\n",
    "    nome_do_arquivo_saida,\n",
    "    sheet_name='Dados de Sinistros',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Arquivo '{nome_do_arquivo_saida}' gerado com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
