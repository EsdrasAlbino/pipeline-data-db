# Pipeline de Dados - Acidentes de TrÃ¢nsito (2019-2021) ğŸš—ğŸ’¥

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa um pipeline completo de integraÃ§Ã£o e consolidaÃ§Ã£o de dados de acidentes de trÃ¢nsito de trÃªs anos consecutivos (2019, 2020 e 2021). O objetivo principal Ã© criar uma base de dados Ãºnica, coesa e padronizada que permita anÃ¡lises aprofundadas, consultas complexas e geraÃ§Ã£o de insights relevantes sobre acidentes de trÃ¢nsito.

## ğŸ¯ Objetivos

- **IntegraÃ§Ã£o de Dados**: Consolidar trÃªs conjuntos de dados distintos de diferentes perÃ­odos temporais
- **PadronizaÃ§Ã£o**: Criar uma estrutura de dados uniforme e consistente
- **AnÃ¡lise Temporal**: Facilitar comparaÃ§Ãµes e anÃ¡lises ao longo do tempo
- **ImplementaÃ§Ã£o de ETL/ELT**: Aplicar tÃ©cnicas modernas de processamento de dados

## ğŸ—‚ï¸ Estrutura do Projeto

```
pipeline-data-db/
â”œâ”€â”€ ETL_Crash.ipynb          # Notebook principal com pipelines ETL e ELT
â”œâ”€â”€ README.md                # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ data 
â”œâ”€â”€â”€â”€ acidentes-2019.csv  
â”œâ”€â”€â”€â”€ acidentes_2020-novo.csv 
â”œâ”€â”€â”€â”€ acidentes2021.csv        
â”œâ”€â”€ banco_etl.db             # Banco SQLite resultante do processo ETL
â”œâ”€â”€ banco_elt.db             # Banco SQLite resultante do processo ELT
â””â”€â”€ acidentes_unificados_2019-2021.csv  # CSV consolidado
```

## ğŸ”§ Tecnologias Utilizadas

- **Python**: Linguagem principal do projeto
- **Pandas**: ManipulaÃ§Ã£o e anÃ¡lise de dados
- **NumPy**: OperaÃ§Ãµes numÃ©ricas
- **SQLAlchemy**: Interface para bancos de dados
- **SQLite**: Banco de dados relacional
- **Jupyter Notebook**: Ambiente de desenvolvimento interativo

## ğŸ“Š Bases de Dados

### Fonte dos Dados
- **Origem**: Dados de acidentes de trÃ¢nsito de Ã³rgÃ£os pÃºblicos
- **PerÃ­odo**: 2019, 2020 e 2021
- **Formato Original**: CSV com separador ';'
- **CodificaÃ§Ã£o**: UTF-8

### Estrutura das Bases Originais

#### Colunas Principais:
- **Temporais**: `data`, `hora`, `DATA` (variaÃ§Ãµes entre anos)
- **LocalizaÃ§Ã£o**: `endereco`, `bairro`, `numero`, `complemento`
- **VeÃ­culos Envolvidos**: `auto`, `moto`, `ciclom`, `ciclista`, `pedestre`, `onibus`, `caminhao`, `viatura`, `outros`
- **VÃ­timas**: `vitimas`, `vitimasfatais`
- **Infraestrutura**: `num_semaforo`, `situacao_semaforo`, `sinalizacao`
- **CondiÃ§Ãµes**: `natureza_acidente`, `tempo_clima`, `condicao_via`

## ğŸ”„ Processos Implementados

### 1. Pipeline ETL (Extract, Transform, Load)

#### **ExtraÃ§Ã£o (Extract)**
```python
# Carregamento dos trÃªs conjuntos de dados
df_2019 = pd.read_csv('acidentes-2019.csv', delimiter=';')
df_2020 = pd.read_csv('acidentes_2020-novo.csv', delimiter=';')
df_2021 = pd.read_csv('acidentes2021.csv', delimiter=';')
```

#### **TransformaÃ§Ã£o (Transform)**
1. **PadronizaÃ§Ã£o de Colunas**:
   - RenomeaÃ§Ã£o de `DATA` para `data` (2019)
   - RemoÃ§Ã£o de colunas inconsistentes entre anos

2. **Tratamento Temporal**:
   - CorreÃ§Ã£o de horÃ¡rios invÃ¡lidos (`24:00:00` â†’ `00:00:00`)
   - CriaÃ§Ã£o de coluna `timestamp` unificada
   - AdiÃ§Ã£o de coluna `ano`

3. **PadronizaÃ§Ã£o de Tipos**:
   - ConversÃ£o de colunas numÃ©ricas para inteiros
   - Tratamento de valores nulos/vazios
   - NormalizaÃ§Ã£o de endereÃ§os

#### **Carga (Load)**
- UnificaÃ§Ã£o dos DataFrames
- ExportaÃ§Ã£o para CSV consolidado
- InserÃ§Ã£o em banco SQLite

### 2. Pipeline ELT (Extract, Load, Transform)

#### **ExtraÃ§Ã£o e Carga**
```python
# Carregamento direto para banco de dados
for arquivo in lista_arquivos_csv:
    df_temp = pd.read_csv(arquivo, sep=';', encoding='utf-8')
    df_temp['ano_do_dado'] = extrair_ano(arquivo)
    df_temp.to_sql('dados_brutos_sinistros', engine, if_exists='append')
```

#### **TransformaÃ§Ã£o no Banco**
```sql
-- CriaÃ§Ã£o de coluna timestamp
ALTER TABLE dados_brutos_sinistros ADD COLUMN TimeStamp TEXT;

-- Tratamento de dados temporais
UPDATE dados_brutos_sinistros 
SET TimeStamp = coalesce(data, DATA) || ' ' || 
    CASE WHEN hora = '24:00:00' THEN '00:00:00' ELSE hora END;

-- PadronizaÃ§Ã£o de campos numÃ©ricos
UPDATE dados_brutos_sinistros 
SET numero = CASE 
    WHEN numero IS NULL OR TRIM(numero) = '' THEN -1 
    ELSE numero 
END;
```

## ğŸ“ˆ Comparativo ETL vs ELT

| Aspecto | ETL | ELT |
|---------|-----|-----|
| **Processamento** | TransformaÃ§Ã£o antes da carga | TransformaÃ§Ã£o apÃ³s a carga |
| **Performance** | Melhor para datasets pequenos | Melhor para grandes volumes |
| **Flexibilidade** | Menor flexibilidade posterior | Maior flexibilidade para ajustes |
| **Recursos** | Usa recursos da aplicaÃ§Ã£o | Usa recursos do banco de dados |
| **ManutenÃ§Ã£o** | CÃ³digo Python mais complexo | SQL mais direto e legÃ­vel |

### Vantagens Observadas

**ETL:**
- âœ… Controle granular das transformaÃ§Ãµes
- âœ… ValidaÃ§Ã£o prÃ©via dos dados
- âœ… Melhor para lÃ³gicas complexas em Python

**ELT:**
- âœ… Performance superior para grandes volumes
- âœ… Aproveitamento da otimizaÃ§Ã£o do SGBD
- âœ… Maior flexibilidade para ajustes posteriores
- âœ… Rastreabilidade completa dos dados brutos

## ğŸš€ Como Executar o Projeto

### PrÃ©-requisitos
```bash
pip install pandas numpy sqlalchemy openpyxl
```

### ExecuÃ§Ã£o

1. **Clone o repositÃ³rio**:
```bash
git clone https://github.com/EsdrasAlbino/pipeline-data-db.git
cd pipeline-data-db
```

2. **Prepare os dados**:
   - Coloque os arquivos CSV na pasta raiz do projeto
   - Certifique-se de que os nomes dos arquivos correspondem aos esperados

3. **Execute o notebook**:
   - Abra `ETL_Crash.ipynb` no Jupyter Notebook ou Google Colab
   - Execute as cÃ©lulas sequencialmente

### Estrutura de ExecuÃ§Ã£o

1. **CÃ©lula 1**: InstalaÃ§Ã£o e importaÃ§Ã£o das dependÃªncias
2. **CÃ©lula 2**: Pipeline ETL completo
3. **CÃ©lula 3**: Pipeline ELT completo
4. **CÃ©lula 4**: GeraÃ§Ã£o de arquivo Excel consolidado

## ğŸ“Š Resultados Esperados

### Arquivos Gerados
- `banco_etl.db`: Banco SQLite com dados processados via ETL
- `banco_elt.db`: Banco SQLite com dados processados via ELT
- `acidentes_unificados_2019-2021.csv`: CSV consolidado
- `dados_brutos_sinistros_consolidados.xlsx`: Excel com anÃ¡lises

### Estrutura Final dos Dados
- **Registros**: ~45.000+ acidentes consolidados
- **PerÃ­odo**: 2019-2021
- **Colunas**: ~25 campos padronizados
- **Qualidade**: Dados limpos e consistentes

## ğŸ” AnÃ¡lises PossÃ­veis

1. **AnÃ¡lise Temporal**: EvoluÃ§Ã£o dos acidentes ao longo dos anos
2. **AnÃ¡lise GeogrÃ¡fica**: DistribuiÃ§Ã£o por bairros e endereÃ§os
3. **AnÃ¡lise de Gravidade**: RelaÃ§Ã£o entre tipos de veÃ­culos e fatalidades
4. **AnÃ¡lise de Infraestrutura**: Impacto de semÃ¡foros e sinalizaÃ§Ã£o
5. **AnÃ¡lise MeteorolÃ³gica**: InfluÃªncia das condiÃ§Ãµes climÃ¡ticas

## ğŸ‘¥ ContribuiÃ§Ãµes

### Como Contribuir
1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add: nova funcionalidade'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### PadrÃµes de Commit
- `Add:` para novas funcionalidades
- `Fix:` para correÃ§Ãµes de bugs
- `Update:` para atualizaÃ§Ãµes e melhorias
- `Remove:` para remoÃ§Ãµes de cÃ³digo
- `Docs:` para atualizaÃ§Ãµes de documentaÃ§Ã£o

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.


**Universidade**: [Nome da Universidade]  
**Disciplina**: Pipeline de Dados  
**Semestre**: [Semestre/Ano]

---

â­ **Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela!**
